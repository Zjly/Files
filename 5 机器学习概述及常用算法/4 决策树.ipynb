{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是决策树\n",
    "\n",
    "* 决策树是对样本数据进行树形分类的过程，由节点和有向边组成。\n",
    "* 节点分为内部节点和叶节点，每个内部节点表示一个特征或属性，叶节点表示类别。\n",
    "* 从根节点开始，样本被分到不同的子节点中，再根据子节点的特征进一步划分。重复这个过程，直到所有样本都被归到某一个类别（叶节点）中。\n",
    "<img src=\"imgs/jcs.png\" width=\"500\">\n",
    "\n",
    "常用的决策树算法有ID3、C4.5、C5.0及CART（Classification And Regression Tree）。<br><br>\n",
    "决策树作为最基础、最常见的监督学习模型，具有简单直观、解释性强的特点，常用于分类问题和回归问题，在市场营销和生物医药领域尤其受欢迎，因为树形结构于与销售、诊断等场景下的决策过程十分相似。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的生成\n",
    "\n",
    "<img src=\"imgs/scjcs.png\" width=\"500\">\n",
    "\n",
    "准备工作\n",
    ">明确自变量和因变量<br>\n",
    "确定信息度量的方式：信息增益、信息增益比、基尼指数<br>\n",
    "确定终止条件：纯度、记录条数、循环次数\n",
    "\n",
    "选择特征\n",
    ">得到当前待处理子集<br>\n",
    "计算所有特征信息度量<br>\n",
    "得到当前最佳分类特征\n",
    "\n",
    "创建分支\n",
    ">根据选中特征将当前记录分为不同分支，分支个数取决于算法\n",
    "\n",
    "是否终止\n",
    ">判断是否满足终止条件，满足则退出循环，不满足则继续递归调用\n",
    "\n",
    "生成结果\n",
    ">判断是否需要剪枝，需要则进行适当剪枝，不需要则保存最终结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3——最大信息增益\n",
    "下面我们以ID3算法为例，来学习决定树的构建。首先，我们来学习几个概念：\n",
    "##### 信息熵（entropy）\n",
    "* 信息论之父，克劳德$\\cdot$香农提出\n",
    "* 描述信息的混论程度\n",
    "* 取值范围是[0,1]，值越大，越混乱，不确定性越大\n",
    "* 对于样本集合D，类别数为n，D的熵的计算公式：\n",
    "$$H(D)=-\\sum _{i=1}^np_i\\log_2p_i$$\n",
    " $p_i$表示该分类出现的概率，式中对数一般取2为底，单位为比特。\n",
    "\n",
    "##### 条件熵\n",
    "* 某个特征A对于数据集D的条件熵的计算公式为：\n",
    "$$ H(D|A)=\\sum _{i=1}^n \\frac{|D_i|}{|D|}H(D_i)\n",
    "=\\sum _{i=1}^n \\frac{|D_i|}{|D|} \\left(-\\sum  _{i=1}^k \\frac{|D_{ik}|}{|D_i|} \\log_2 \\frac{|D_{ik}|}{|D_i|} \\right)$$\n",
    "$D_i$表示$D$中特征$A$取第$i$个值的样本子集，$D_{ik}$表示$D_i$中属于第$k$类的样本子集。<br><br>\n",
    "* 其实条件熵就是按一个新变量的每个值对原变量进行分类，然后在每一个小类里面计算一个小熵，再将每一个小熵乘以对应类别的概率，然后求和。\n",
    "\n",
    "##### 信息增益（information gain）\n",
    "* 信息是确定性增加\n",
    "* 信息增益是从一个状态到另一个状态信息的变化值\n",
    "* 信息增益越大，对确定性贡献越大\n",
    "* 计算公式为：\n",
    "$$g(D,A)=H(D)-H(D|A)$$\n",
    "\n",
    "在刚才的例子中：\n",
    ">总样本数5，不见的概率$\\frac{3}{5}$，见的概率$\\frac{2}{5}$,D的信息熵：\n",
    "$$\n",
    "H(D)=-\\frac{3}{5}\\log_2\\frac{3}{5}-\\frac{2}{5}\\log_2\\frac{2}{5}=0.971\n",
    "$$<br>\n",
    "年龄：老$\\frac{1}{5}$，年轻$\\frac{4}{5}$，年龄的条件熵：\n",
    "$$\n",
    "H(D|年龄)=\\frac{1}{5}H(老)+\\frac{4}{5}H(年轻)\n",
    "=\\frac{1}{5}(-1\\log_21)+\\frac{4}{5}(-\\frac{2}{4}\\log_2\\frac{2}{4}-\\frac{2}{4}\\log_2\\frac{2}{4})=0.8\n",
    "$$<br>\n",
    "长相：帅$\\frac{1}{5}$，一般$\\frac{3}{5}$，丑$\\frac{1}{5}$，长相的条件熵：\n",
    "$$\n",
    "H(D|长相)=\\frac{1}{5}H(帅)+\\frac{3}{5}H(一般)+\\frac{1}{5}H(丑)\n",
    "=\\frac{1}{5}(-1\\log_21)+\\frac{3}{5}(-\\frac{2}{3}\\log_2\\frac{2}{3}-\\frac{1}{3}\\log_2\\frac{1}{3})+\\frac{1}{5}(-1\\log_21)=0.551\n",
    "$$<br>\n",
    "工资：高$\\frac{3}{5}$，中等$\\frac{1}{5}$，低$\\frac{1}{5}$，工资的条件熵：\n",
    "$$\n",
    "H(D|工资)=\\frac{3}{5}H(高)+\\frac{1}{5}H(中等)+\\frac{1}{5}H(低)\n",
    "=\\frac{3}{5}(-\\frac{2}{3}log_2\\frac{2}{3}-\\frac{1}{3}log_2\\frac{1}{3})+\\frac{1}{5}\\times0+\\frac{1}{5}\\times0=0.551\n",
    "$$\n",
    "程序员：是$\\frac{2}{5}$，不是$\\frac{3}{5}$，程序员的条件熵：\n",
    "$$\n",
    "H(D|程序员)=\\frac{2}{5}H(是)+\\frac{3}{5}H(不是)\n",
    "=\\frac{2}{5}(-\\frac{2}{2}log_2\\frac{2}{2})+\\frac{3}{5}(-\\frac{3}{3}log_2\\frac{3}{3})=0\n",
    "$$<br>\n",
    "所以，各个特征的信息增益为：<br>\n",
    "$g(D|年龄)=H(D)-H(D|年龄)=0.971-0.8=0.171$ <br>\n",
    "$g(D|长相)=H(D)-H(D|长相)=0.971-0.551=0.420$<br>\n",
    "$ g(D|工资)=H(D)-H(D|工资)=0.971-0.551=0.420 $<br>\n",
    "$g(D|程序员)=H(D)-H(D|程序员)=0.971-0=0.971$\n",
    "\n",
    "显然“程序员”的信息增益最大，所以选择它作为决策树的根节点。\n",
    "<img src=\"imgs/jcs1.png\" width=\"500\">\n",
    "而经过此逆天特征后，各分支的经验熵均为$-1 \\times \\log_21=0$，完成了决策树生长。当然，实际应用中，决策树往往不能通过一个特征就完成构建，需要在经验熵不为0的分支中继续生长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709505944546686\n",
      "0.5509775004326938\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#?math.log\n",
    "print(- 3/5 * math.log(3/5,2) - 2/5 * math.log(2/5,2))\n",
    "a = - 2/3 * math.log(2/3,2)- 1/3 * math.log(1/3,2)\n",
    "print(a * 3/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5——最大信息增益率\n",
    "\n",
    "* 信息增益率定义为：\n",
    "$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$\n",
    "* 其中\n",
    "$$ H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|} \\log_2\\frac{|D_i|}{|D|}$$\n",
    "叫做数据集D关于A的取值熵。\n",
    "\n",
    "现在我们来计算上例中每个特征的取值熵：\n",
    "> $$ H_{年龄}(D)=-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{4}{5}log_2\\frac{4}{5}=0.722$$\n",
    "$$ H_{长相}(D)=-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{1}{5}log_2\\frac{1}{5}=1.371$$\n",
    "$$ H_{工资}(D)=-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{1}{5}log_2\\frac{1}{5}-\\frac{1}{5}log_2\\frac{1}{5}=1.371$$\n",
    "$$ H_{程序员}(D)=-\\frac{3}{5}log_2\\frac{3}{5}-\\frac{2}{5}log_2\\frac{2}{5}=0.971$$<br>\n",
    "所以，各个特征的信息增益率为：<br>\n",
    "$g_R(D,年龄)=\\frac{0.171}{0.722}=0.236$<br>\n",
    "$g_R(D,长相)=\\frac{0.420}{1.371}=0.402$<br>\n",
    "$g_R(D,工资)=\\frac{0.420}{1.371}=0.402$<br>\n",
    "$g_R(D,程序员)=\\frac{0.971}{0.971}=1$<br>\n",
    "\n",
    "信息增益率最大的依然是“程序员”，但是对比信息增益，“年龄”对应的指标上升了，“长相”“工资”对应的指标有所下降。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7219280948873623\n",
      "1.3709505944546687\n",
      "0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "print(- 1/5 * math.log(1/5,2) - 4/5 * math.log(4/5,2))\n",
    "print(- 1/5 * math.log(1/5,2) - 3/5 * math.log(3/5,2)- 1/5 * math.log(1/5,2))\n",
    "print(- 3/5 * math.log(3/5,2) - 2/5 * math.log(2/5,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART——最小基尼指数\n",
    "\n",
    "* Gini描述的是数据的纯度，与信息熵含义类似。\n",
    "* 值越大，不确定性越大\n",
    "* 公式为：\n",
    "$$Gini(D)=1-\\sum_{i=1}^np_i^2$$\n",
    "* 特征A的Gini指数定义为：\n",
    "$$Gini(D|A)=\\sum_{i=1}^n \\frac{|D_i|}{|D|}Gini(D_i)$$\n",
    "\n",
    "CART每次迭代时选择基尼指数最小的特征及其对应的切分点进行分类，不过与ID3、C4.5不同的是，CART是一颗二叉树，每次将数据按特征A的取值切成两份，分别进入左右子树。\n",
    "解释一下就是：\n",
    "> 数据集D中A若为离散特征，则根据A的某一可能值a将D分为$D_1$与$D_2$<br><br>\n",
    "$$D_1=\\{D|A=a\\}\\,\\,\\,D_2=\\{D|A \\neq a\\}$$<br>\n",
    "$$Gini(D|A)=\\frac{|D_1|}{|D|}Gini(D_1) +\\frac{|D_2|}{|D|}Gini(D_2) $$\n",
    "\n",
    "现在，我们来计算各个特征的基尼指数：\n",
    ">Gini(D|年龄=老)=$\\frac{1}{5}(1-1)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4$<br>\n",
    "Gini(D|年龄=年轻)=$\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)+\\frac{1}{5}(1-1)=0.4$<br>\n",
    "Gini(D|长相=帅)=$\\frac{1}{5}(1-1)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4$<br>\n",
    "Gini(D|长相=丑)=$\\frac{1}{5}(1-1)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4$<br>\n",
    "Gini(D|长相=一般)=$\\frac{3}{5}(1-(\\frac{2}{3})^2-(\\frac{1}{3})^2)+\\frac{2}{5}(1-(\\frac{2}{2})^2)=0.267$<br>\n",
    "Gini(D|工资=高)=$\\frac{3}{5}(1-(\\frac{2}{3})^2-(\\frac{1}{3})^2)+\\frac{2}{5}(1-(\\frac{1}{2})^2-(\\frac{1}{2})^2)=0.467$<br>\n",
    "Gini(D|工资=中等)=$\\frac{1}{5}(1-1)+\\frac{4}{5}(1-(\\frac{1}{4})^2-(\\frac{3}{4})^2)=0.3$<br>\n",
    "Gini(D|工资=低)=$\\frac{1}{5}(1-1)+\\frac{4}{5}(1-(\\frac{2}{4})^2-(\\frac{2}{4})^2)=0.4$<br>\n",
    "Gini(D|程序员=是)=$\\frac{2}{5}(1-1)+\\frac{3}{5}(1-1)=0$<br>\n",
    "Gini(D|程序员=不是)=$\\frac{3}{5}(1-1)+\\frac{2}{5}(1-1)=0$\n",
    "\n",
    "我们很快发现特征“程序员”的Gini指数最小值为0，所以“程序员”作为最优特征，“程序员=是”为最优切分点。按照这种切分，从根节点会直接产生两个叶节点，基尼系数降为0，完成决策树生长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 几种算法的总结\n",
    "\n",
    "### ID3系列\n",
    "\n",
    "#### ID3（Iterative Dichotomiser3）\n",
    "* 核心是信息熵、根据信息增益决定树的节点\n",
    "* 分类是多叉树\n",
    "* 缺点：\n",
    "    + 信息度量不合理，倾向于选择取值多的字段\n",
    "    + 输入类型单一，只支持离散型\n",
    "    + 不做剪枝，容易过拟合\n",
    "    \n",
    "#### C4.5（Classifier 4.5）\n",
    "对比ID3有以下改进：\n",
    "* 用信息增益率代替信息增益\n",
    "* 能对连续属性进行离散化、对缺失数据进行处理\n",
    "* 悲观剪枝法\n",
    "\n",
    "#### C5.0（Classifier 5.0）\n",
    "对比C4.5有以下改进：\n",
    "* 使用了Boosting（集成学习的一种方法）\n",
    "\n",
    "### CART\n",
    "* 核心是基尼指数\n",
    "* 分类是二叉树\n",
    "* 支持连续值和离散值\n",
    "* 代价复杂度剪枝\n",
    "* 支持回归，可预测连续值\n",
    "\n",
    "## 决策树剪枝\n",
    "为了避免过拟合（在测试集表现得很差），我们需要对完全生长的决策树进行修枝，减掉一些次要属性，提高模型的泛化能力。常用的方法有两种：\n",
    "\n",
    "* 预剪枝（Pre-Pruning）：在生成决策树的过程中提前停止树的增长\n",
    "    + 当树到达一定深度的时候，停止树的生长\n",
    "    + 当到达当前节点的样本数量大于某个阈值的时候，停止树的生长\n",
    "    + 计算每次分裂对测试集的准确度提升，当小于某个阈值的时候，停止树的生长\n",
    "    + 优点：思想直接、算法简单、效率高、适合解决大规模问题\n",
    "    + 缺点：准确的停止树的生长需要经验判断、有欠拟合的风险\n",
    "    \n",
    "    \n",
    "* 后剪枝（Post-Pruning）：在已生成的过拟合的决策树上进行剪枝\n",
    "    + 核心思想：从“发育”完全的决策树的底层向上计算是否剪枝。剪枝是将子树删除，用一个叶节点代替，该节点的类型按照多数投票的原则判断。也可以通过在测试集上的准确率进行判断，如果剪枝后准确率有所提升则进行剪枝。\n",
    "    + 常见后剪枝算法：\n",
    "        - 错误率降低剪枝（Reduced Error Pruning,REP）\n",
    "        - 悲观剪枝（Pessimistic Error Pruning,PEP）\n",
    "        - 代价复杂度剪枝（Cost Complexity Pruning,CCP）\n",
    "        - 最小误差剪枝（Minimum Error Pruning,MEP）\n",
    "        - ……\n",
    "    + 优点：泛化能力更强\n",
    "    + 缺点：时间开销更大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成学习（Ensemble Learning）\n",
    "\n",
    "前面我们学习的算法都是一种单一的模型，有时候可能单一模型的表现可能并不理想。集成学习的思想就如同贤明的君王，听取众谋臣的建议，最后综合考虑，得出最终决策。\n",
    "* 集成学习：针对同一数据集，用多种分类器训练，汇总得出最终结果。这其中每个单独的分类器称为基分类器。\n",
    "* 主要分为两种：\n",
    "    + Bagging（Bootstrap Aggregating）\n",
    "        - 有放回抽样构建多个子集\n",
    "        - 训练多个分类器\n",
    "        - 训练是并行的\n",
    "        - 最终结果由各分类器结果投票得出\n",
    "        - 代表算法：随机森林（Random Forest,RF）\n",
    "        - <img src=\"imgs/bagging.png\" width=\"500\">\n",
    "    + Boosting\n",
    "        - 重复使用一类分类器来修改训练集\n",
    "        - 每次训练后根据结果调整样本的权重\n",
    "        - 训练是串行的\n",
    "        - 每个分类器加权后的线性组合为最终结果\n",
    "        - 代表算法：自适应元算法（Adaptive boosing,Adaboost）、梯度提升决策树（Gradient Boosting Decision Tree,GBDT）\n",
    "        - <img src=\"imgs/boosting.png\" width=\"550\">\n",
    "\n",
    "一句话总结就是——<span style=\"color:red;\">“三个臭皮匠，顶个诸葛亮”</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 计算给定数据集的香农熵\n",
    "from  math import log\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    labels = ['no surfacing', 'flippers']\n",
    "    # change to discrete values\n",
    "    return dataSet, labels\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    numEntries = len(dataSet)  # 样本数\n",
    "    labelCounts = {}   # 创建一个数据字典：key是最后一列的数值（即标签），value是属于该类别的样本个数\n",
    "    for featVec in dataSet: # 遍历整个数据集，每次取一行\n",
    "        currentLabel = featVec[-1]  #取该行最后一列的值\n",
    "        if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] += 1\n",
    "        \n",
    "    shannonEnt = 0.0  # 初始化信息熵\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        shannonEnt -= prob * log(prob,2) #log base 2  计算信息熵\n",
    "    return shannonEnt\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis]  # chop out axis used for splitting\n",
    "            reducedFeatVec.extend(featVec[axis + 1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    numFeatures = len(dataSet[0]) - 1  # the last column is used for the labels\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0;\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):  # iterate over all the features\n",
    "        featList = [example[i] for example in dataSet]  # create a list of all the examples of this feature\n",
    "        uniqueVals = set(featList)  # get a set of unique values\n",
    "        newEntropy = 0.0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy  # calculate the info gain; ie reduction in entropy\n",
    "        if (infoGain > bestInfoGain):  # compare this to the best gain so far\n",
    "            bestInfoGain = infoGain  # if better than current best, set to best\n",
    "            bestFeature = i\n",
    "    return bestFeature  # returns an integer\n",
    "\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote] = 0\n",
    "        classCount[vote] += 1\n",
    "    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]  # stop splitting when all of the classes are equal\n",
    "    if len(dataSet[0]) == 1:  # stop splitting when there are no more features in dataSet\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    del (labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]  # copy all of labels, so trees don't mess up existing labels\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "    return myTree\n",
    "\n",
    "\n",
    "\n",
    "## 下面是可视化决策树\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")\n",
    "leafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\n",
    "arrow_args = dict(arrowstyle=\"<-\")\n",
    "\n",
    "def getNumLeafs(myTree):\n",
    "    numLeafs = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[\n",
    "                    key]).__name__ == 'dict':  # test to see if the nodes are dictonaires, if not they are leaf nodes\n",
    "            numLeafs += getNumLeafs(secondDict[key])\n",
    "        else:\n",
    "            numLeafs += 1\n",
    "    return numLeafs\n",
    "\n",
    "\n",
    "def getTreeDepth(myTree):\n",
    "    maxDepth = 0\n",
    "    firstStr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firstStr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[\n",
    "                    key]).__name__ == 'dict':  # test to see if the nodes are dictonaires, if not they are leaf nodes\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:\n",
    "            thisDepth = 1\n",
    "        if thisDepth > maxDepth: maxDepth = thisDepth\n",
    "    return maxDepth\n",
    "\n",
    "\n",
    "def plotNode(nodeTxt, centerPt, parentPt, nodeType):\n",
    "    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords='axes fraction',\n",
    "                            xytext=centerPt, textcoords='axes fraction',\n",
    "                            va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n",
    "    \n",
    "    \n",
    "def plotMidText(cntrPt, parentPt, txtString):\n",
    "    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]\n",
    "    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]\n",
    "    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n",
    "\n",
    "\n",
    "def plotTree(myTree, parentPt, nodeTxt):  # if the first key tells you what feat was split on\n",
    "    numLeafs = getNumLeafs(myTree)  # this determines the x width of this tree\n",
    "    depth = getTreeDepth(myTree)\n",
    "    firstStr = list(myTree.keys())[0]  # the text label for this node should be this\n",
    "    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff)\n",
    "    plotMidText(cntrPt, parentPt, nodeTxt)\n",
    "    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n",
    "    secondDict = myTree[firstStr]\n",
    "    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[\n",
    "                    key]).__name__ == 'dict':  # test to see if the nodes are dictonaires, if not they are leaf nodes\n",
    "            plotTree(secondDict[key], cntrPt, str(key))  # recursion\n",
    "        else:  # it's a leaf node print the leaf node\n",
    "            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW\n",
    "            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n",
    "            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n",
    "    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD\n",
    "    \n",
    "    \n",
    "def createPlot(inTree):\n",
    "    fig = plt.figure(1, facecolor='white')\n",
    "    fig.clf()\n",
    "    axprops = dict(xticks=[], yticks=[])\n",
    "    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)  # no ticks\n",
    "    # createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses\n",
    "    plotTree.totalW = float(getNumLeafs(inTree))\n",
    "    plotTree.totalD = float(getTreeDepth(inTree))\n",
    "    plotTree.xOff = -0.5 / plotTree.totalW;\n",
    "    plotTree.yOff = 1.0;\n",
    "    plotTree(inTree, (0.5, 1.0), '')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "data,label=createDataSet()\n",
    "# print(data)\n",
    "tr=createTree(data,label)\n",
    "print(tr)\n",
    "\n",
    "createPlot(tr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
